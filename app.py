# -*- coding: utf-8 -*-
"""newapp.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D_SVhXGpyUoqqhRs_SFWQQpD5Qd5CDrK
"""

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
from transformers import GPT2Tokenizer
import os
import json
from groq import Groq

# Groq API Configuration
CONFIG = {
    "model_id": "mixtral-8x7b-32768"
}

def get_groq_client():
    """Retrieves a Groq API client."""
    api_key = os.environ.get("GROQ_API_KEY")
    if not api_key:
        raise ValueError("GROQ_API_KEY not found in environment variables.")
    return Groq(api_key=api_key)

def categorize_momentum(score):
    """Categorizes momentum score into tiers."""
    score = float(score)
    if score <= 150:
        return 'Tier 1: Ambient Noise (Normal baseline activity)'
    elif score <= 500:
        return 'Tier 2: Emerging Narrative (Potential story development)'
    elif score <= 2000:
        return 'Tier 3: Coordinated Activity (Organized group behavior)'
    else:
        return 'Tier 4: Viral Emergency (Requires immediate response)'

def generate_investigative_report(cluster_data, momentum_states, cluster_id, max_tokens=1024):
    """Generates a FIMI intelligence report for Gabon's election, using top documents and metrics."""
    client = get_groq_client()
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2-xl")
    metrics = momentum_states.get(cluster_id, {})
    sample_docs = cluster_data[['text', 'URL', 'Timestamp']].values.tolist()
    random.shuffle(sample_docs)
    selected_docs, total_tokens = [], 0

    for doc in sample_docs:
        doc_tokens = len(tokenizer.encode(doc[0]))
        if total_tokens + doc_tokens <= max_tokens:
            selected_docs.append(doc); total_tokens += doc_tokens
        else: break

    prompt = f"""Generate a Gabon election FIMI report:
    - Context, narratives, lifecycles ({cluster_data['Timestamp'].min().strftime('%Y-%m-%d %H:%M')} to {cluster_data['Timestamp'].max().strftime('%Y-%m-%d %H:%M')}), vehicles, and sources.
    - Analyze Russia/China/Turkey/Saudi Arabia ties, extremism, anti-West/France, pro/anti-ECOWAS/AES, pro-Russia/China sentiment.
    - Identify hate speech, coordinated networks (metrics: {metrics}, sources: {cluster_data['Source'].nunique()}, momentum: {metrics.get('momentum', 'N/A'):.2f}), crossposting, narrative engineering, AI content, manipulated media, linguistic fingerprints.
    - Suggest 2-3 investigative leads.
    - Use provided document URLs as evidence, exclude speculation, and focus on election-related content.
    { "".join([f"Document {i+1}: {doc[0]}\nURL: {doc[1]}\n[TIMESTAMP]: {doc[2]}" for i, doc in enumerate(selected_docs)]) }
    """

    response = client.chat.completions.create(model=CONFIG["model_id"], messages=[{"role": "system", "content": prompt}], temperature=0.6, max_tokens=800)
    return {"report": response.choices[0].message.content, "metrics": metrics, "sample_texts": [doc[0] for doc in selected_docs], "sample_urls": [doc[1] for doc in selected_docs], "Time": [doc[2] for doc in selected_docs], "all_urls": cluster_data['URL'].head(20).tolist(), "source_count": cluster_data['Source'].nunique(), "momentum_score": cluster_data['momentum_score'].iloc[0]}

def main():
    st.title("Radar Gabon Results")

    try:
        df = pd.read_csv("Radar_Gabon.csv")

        # Assuming 'cluster_id' exists in your CSV
        cluster_ids = df['cluster_id'].unique()
        selected_cluster_id = st.selectbox("Select Cluster ID", cluster_ids)

        cluster_data = df[df['cluster_id'] == selected_cluster_id]

        # Calculate momentum states (adjust as needed)
        momentum_states = {selected_cluster_id: {
            'cumulative_activity': len(cluster_data),
            'peak_activity': len(cluster_data),
            'sources': cluster_data['Source'].nunique(),
            'momentum': cluster_data['momentum_score'].iloc[0] if not cluster_data.empty else 0.0
        }}

        report_data = generate_investigative_report(cluster_data, momentum_states, selected_cluster_id)

        st.write("## Investigative Report")
        st.write(report_data['report'])

        st.write("## Sample Documents")
        for i, text in enumerate(report_data['sample_texts']):
            st.write(f"Document {i+1}: {text}")
            st.write(f"URL: {report_data['sample_urls'][i]}")
            st.write(f"Timestamp: {report_data['Time'][i]}")

        st.write("## Momentum Visualization")
        plt.figure(figsize=(10, 4))
        sns.lineplot(x=pd.to_datetime(cluster_data['Timestamp']), y=cluster_data['momentum_score'])
        plt.title("Momentum Over Time")
        st.pyplot(plt)

        st.write("## Momentum Score Category")
        st.write(categorize_momentum(report_data['momentum_score']))

        st.write("## Other Metrics")
        st.write(f"Total Posts: {report_data['metrics'].get('cumulative_activity', 'N/A')}")
        st.write(f"Peak Hourly Activity: {report_data['metrics'].get('peak_activity', 'N/A')}")
        st.write(f"Unique Sources: {report_data['source_count']}")

    except FileNotFoundError:
        st.error("Radar_Gabon.csv not found. Please ensure it's in the same directory.")
    except Exception as e:
        st.error(f"An error occurred: {e}")

if __name__ == "__main__":
    main()