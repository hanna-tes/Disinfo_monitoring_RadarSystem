# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hkSMCjYIaZQcce1JDJS_IrJIqPLDnSw
"""

# pipeline.py
# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hkSMCjYIaZQcce1JDJS_IrJIqPLDnSw
"""

# pipeline.py
import torch
torch._dynamo.config.suppress_errors = True  # Disable Dynamo error propagation
#torch._dynamo.config.disable = True  # Completely disable TorchDynamo
#torch.set_float32_matmul_precision('high')  # Optimize for MPS/MacOS
import os
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, GPT2Tokenizer
from sklearn.decomposition import PCA
import annoy
from bertopic import BERTopic
from hdbscan import HDBSCAN
import logging
from groq import Groq
import re
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import seaborn as sns
import streamlit as st
import random


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Configuration (now using Streamlit secrets)
CONFIG = {
    "api_key": "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"
    "model_id": "llama3-70b-8192",
    "gpu_params": {
        "batch_size": 256,  # Increased batch size
        "max_seq_length": 128,
        "num_workers": 4,
        "fp16": True  # Enable mixed precision
    },
    "bertrend": {
        "model_name": "bert-base-multilingual-cased", # Explicitly set model name
        "temporal_weight": 0.5,
        "cluster_threshold": 0.35,  # Adjusted to a reasonable value
        "min_cluster_size": 4,
        "growth_threshold": 1.2,  # Adjusted to a reasonable value
        "pca_components": 32,
        "chunk_size": 100,
        "ann_neighbors": 25,
        "time_window_hours": 24  # Reduced time window
    },
    "analysis": { # Corrected indentation here
        "time_window": "24H",  # Reduced time window
        "min_sources": 2,
        "decay_factor": 0.015,
        "decay_power": 1.8,
        "visualization": {
            "plot_size": (12, 8),
            "palette": "viridis",
            "max_display_clusters": 10
        }
    }
}
# Initialize Groq client with Streamlit secrets
def get_groq_client():
    return Groq(api_key=st.secrets.groq.api_key)

# Load BERT model to GPU
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)

# Initialize GPU with mixed precision
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = torch.cuda.amp.GradScaler(enabled=CONFIG["gpu_params"]["fp16"])
logger.info(f"Using device: {device}")

# Optimized BERT Model Loading
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)
#bert_model = torch.compile(bert_model)  # Enable model compilation

# GPU-optimized Dataset with Pre-batching
class DRCDataset(Dataset):
    def __init__(self, texts):
        # Ensure all texts are strings and filter invalid entries
        self.texts = [str(t) for t in texts if isinstance(t, (str, bytes))]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Return empty string for invalid indices
        if idx >= len(self):
            return ""
        return self.texts[idx]

    def collate_fn(self, batch):
        # Filter empty strings and None values
        batch = [text for text in batch if isinstance(text, (str, bytes)) and len(text) > 0]
        
        # Handle empty batches
        if not batch:
            return {"input_ids": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
        
        # Tokenize with error handling
        try:
            return tokenizer(
                batch,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=CONFIG["gpu_params"]["max_seq_length"],
                return_attention_mask=True
            )
        except Exception as e:
            logger.error(f"Tokenizer error: {str(e)}")
            return {"input_ids": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
                    
# Turbo-charged BERT Embeddings Generator
def get_bert_embeddings(texts):
    dataset = DRCDataset(texts)
    dataloader = DataLoader(
        dataset,
        batch_size=CONFIG["gpu_params"]["batch_size"],
        num_workers=0,  # Disable multiprocessing for stability
        collate_fn=dataset.collate_fn,
        pin_memory=True,
        shuffle=False
    )

    embeddings = []
    with torch.no_grad():
        for batch in dataloader:
            # Skip empty batches
            if batch["input_ids"].shape[0] == 0:
                continue
                
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = bert_model(**inputs)
            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu())

    # Combine and reduce dimensionality on GPU
    full_embeddings = torch.cat(embeddings)
    pca = PCA(n_components=CONFIG["bertrend"]["pca_components"])
    return pca.fit_transform(full_embeddings.cpu().numpy())

# GPU-accelerated Temporal-Spatial Distance Calculator
def temporal_distance_matrix(embeddings, timestamps):
    """Hybrid distance calculation with temporal constraints"""
    # Convert to tensors on GPU
    emb_tensor = torch.tensor(embeddings, device=device)
    time_tensor = torch.tensor(timestamps, dtype=torch.float64, device=device)

    # Calculate time differences in hours
    time_diff = torch.abs(time_tensor[:, None] - time_tensor[None, :]) / 3.6e9

    # Apply temporal window mask
    time_mask = (time_diff < CONFIG["bertrend"]["time_window_hours"]).float()

    # Calculate semantic distances
    semantic_dists = torch.cdist(emb_tensor, emb_tensor, p=2)

    # Combine with temporal mask
    combined_dists = (
        CONFIG["bertrend"]["temporal_weight"] * time_diff +
        (1 - CONFIG["bertrend"]["temporal_weight"]) * semantic_dists
    ) * time_mask

    return combined_dists.cpu().numpy()

# Hyper-optimized BERTrend Analysis
def bertrend_analysis(df):
    """GPU-powered clustering pipeline with temporal constraints"""
    st.write("Input DataFrame to bertrend_analysis:")
    st.write(df)
    st.write(f"Input DataFrame shape: {df.shape}")
    st.write(f"Input DataFrame is empty: {df.empty}")

    try:
        logger.info("Generating turbo-charged BERT embeddings...")
        embeddings = get_bert_embeddings(df['text'].tolist())
        timestamps = df['Timestamp'].astype(np.int64).values

        # Create ANN index for fast neighbor search
        ann_index = annoy.AnnoyIndex(embeddings.shape[1], 'euclidean')
        for i, emb in enumerate(embeddings):
            ann_index.add_item(i, emb)
        ann_index.build(20)  # More trees for better accuracy

        # Initialize cluster array
        clusters = np.full(len(df), -1, dtype=int)
        current_cluster = 0

        # Process in large chunks with temporal constraints
        chunk_size = CONFIG["bertrend"]["chunk_size"]
        for i in range(0, len(embeddings), chunk_size):
            chunk_end = min(i + chunk_size, len(embeddings))
            logger.info(f"Processing chunk {i//chunk_size + 1}/{(len(embeddings)//chunk_size)+1}")

            # Get temporal window for current chunk
            chunk_times = timestamps[i:chunk_end]
            time_mask = (timestamps >= chunk_times[0]) & (timestamps <= chunk_times[-1])

            # Find ANN neighbors within temporal window
            neighbor_candidates = set()
            for idx in range(i, chunk_end):
                neighbors = ann_index.get_nns_by_item(
                    idx,
                    CONFIG["bertrend"]["ann_neighbors"],
                    search_k=100000  # Higher search effort
                )
                neighbor_candidates.update(neighbors)

            # Filter candidates by temporal window
            candidate_indices = np.array(list(neighbor_candidates))
            candidate_indices = candidate_indices[time_mask[candidate_indices]]

            # Early exit for no candidates
            if len(candidate_indices) == 0:
                continue

            # Calculate distances only for candidates
            sub_emb = embeddings[candidate_indices]
            sub_ts = timestamps[candidate_indices]
            dist_matrix = temporal_distance_matrix(sub_emb, sub_ts)
            dist_matrix = dist_matrix.astype(np.double)

            # Fast GPU-accelerated clustering
            clusterer = HDBSCAN(
                min_cluster_size=CONFIG["bertrend"]["min_cluster_size"],
                metric="precomputed",
                cluster_selection_epsilon=CONFIG["bertrend"]["cluster_threshold"],
                core_dist_n_jobs=4
            )
            chunk_clusters = clusterer.fit_predict(dist_matrix)

            # Update cluster assignments
            valid_mask = chunk_clusters != -1
            chunk_clusters[valid_mask] += current_cluster
            clusters[candidate_indices] = chunk_clusters
            current_cluster = chunk_clusters[valid_mask].max() + 1 if valid_mask.any() else current_cluster

        df['Cluster'] = clusters
        df = df[df['Cluster'] != -1]

        st.write("DataFrame after clustering:")
        st.write(df)
        st.write(f"DataFrame after clustering shape: {df.shape}")
        st.write(f"DataFrame after clustering is empty: {df.empty}")

        return df

    except Exception as e:
        logger.error(f"Error in bertrend_analysis: {e}")
        st.error(f"Error in bertrend_analysis: {e}")
        return pd.DataFrame() # return empty dataframe so that the app will not crash.

# Optimized Momentum Calculator
def calculate_trend_momentum(clustered_df):
    """Optimized momentum calculation with source tracking"""
    df = clustered_df.copy()

    # Temporal binning
    df['time_window'] = df['Timestamp'].dt.floor(CONFIG["analysis"]["time_window"])

    # Aggregate with source tracking
    grouped = df.groupby(['Cluster', 'time_window']).agg(
        count=('text', 'size'),
        sources=('Source', 'unique'),
        last_time=('Timestamp', 'max')
    ).reset_index()

    # Calculate cumulative sources and momentum
    emerging = []
    momentum_states = {}

    for cluster, cluster_group in grouped.groupby('Cluster'):
        if cluster == -1:
            continue

        cluster_group = cluster_group.sort_values('time_window')
        cumulative_sources = set()
        momentum = 0
        last_update = None

        for idx, row in cluster_group.iterrows():
            # Time decay calculation
            if last_update is not None:
                delta_hours = (row['last_time'] - last_update).total_seconds() / 3600
                decay = np.exp(-CONFIG["analysis"]["decay_factor"] *
                             (delta_hours ** CONFIG["analysis"]["decay_power"]))
                momentum *= decay

            # Update momentum and sources
            momentum += row['count']
            cumulative_sources.update(row['sources'])

            # Calculate momentum score
            momentum_score = momentum * len(cumulative_sources) * np.log1p(row['count'])

            if (momentum_score > CONFIG["bertrend"]["growth_threshold"] and
                len(cumulative_sources) >= CONFIG["analysis"]["min_sources"]):
                emerging.append((cluster, momentum_score))

            last_update = row['last_time']

        momentum_states[cluster] = {
            'momentum': momentum,
            'last_update': last_update,
            'sources': cumulative_sources
        }

    return sorted(emerging, key=lambda x: -x[1]), momentum_states

# ... Keep visualization and report functions similar but ensure they filter data early ...
def visualize_trends(clustered_df, momentum_states):
    """Generate interactive trend visualizations"""
    st.write("DataFrame before pivot_table:")
    st.write(clustered_df)
    st.write(f"DataFrame before pivot_table shape: {clustered_df.shape}")
    st.write(f"DataFrame before pivot_table is empty: {clustered_df.empty}")

    plt.figure(figsize=CONFIG["analysis"]["visualization"]["plot_size"])

    # Timeline Plot
    plt.subplot(2, 1, 1)
    for cluster in list(momentum_states.keys())[:CONFIG["analysis"]["visualization"]["max_display_clusters"]]:
        cluster_data = clustered_df[clustered_df['Cluster'] == cluster]
        timeline = cluster_data.groupby(pd.Grouper(key='Timestamp', freq='6H'))['text'].count().cumsum()
        plt.plot(timeline.index, timeline, label=f"Cluster {cluster}", lw=2, alpha=0.8)

    plt.gca().xaxis.set_major_formatter(DateFormatter('%Y-%m-%d\n%H:%M'))
    plt.title("Narrative Momentum Timeline")
    plt.ylabel("Cumulative Momentum")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)

    # Heatmap
    plt.subplot(2, 1, 2)
    heatmap_data = clustered_df.pivot_table(
        index=pd.Grouper(key='Timestamp', freq='6H'),
        columns='Cluster',
        values='text',
        aggfunc='count',
        fill_value=0
    ).iloc[:, :CONFIG["analysis"]["visualization"]["max_display_clusters"]]

    sns.heatmap(
        heatmap_data.T,
        cmap=CONFIG["analysis"]["visualization"]["palette"],
        cbar_kws={'label': 'Activity Level'}
    )
    plt.title("Cluster Activity Patterns")
    plt.xlabel("Time Windows")
    plt.ylabel("Cluster ID")

    plt.tight_layout()
    #Removed the line that uses CONFIG["data_path"]
    plt.savefig("trend_visualization.png", bbox_inches='tight')
    plt.close()

    logger.info("Visualization saved to trend_visualization.png")
    return "trend_visualization.png"
    
    # Display the plot in Streamlit
    st.pyplot(plt)
    plt.close() # Close the plot to free memory.

    st.write("Visualizations Generated") #Add a message to streamlit.

def generate_investigative_report(cluster_data, momentum_states, cluster_id, max_tokens=1024):
    """Generate report with top 3 documents and their URLs"""
    client = Groq(api_key=CONFIG["api_key"])
    #tokenizer = GPT2Tokenizer.from_pretrained("gpt2-xl")

    try:
        # Get top 10 documents with URLs
        metrics = momentum_states.get(cluster_id, {})
        sample_docs = cluster_data[['text', 'URL', 'Timestamp']].values.tolist()

        random.shuffle(sample_docs)
        Country="Gabon"


        #report_context = f"""
        #Quantitative Context:
        #- Total Posts: {metrics.get('cumulative_activity', 'N/A')}
        #- Peak Hourly Activity: {metrics.get('peak_activity', 'N/A')}
        #- Unique Sources: {metrics.get('sources', 'N/A')}
        #- Current Momentum Score: {metrics.get('momentum', 'N/A'):.2f}
        #"""
        # Initialize the list of documents to include
        selected_docs = []
        total_tokens = 0

        # Select documents until we hit the token limit
        for doc in sample_docs:
            # Calculate the token count for the document
            doc_tokens = len(tokenizer.encode(doc[0]))  # Encoding only the text

            if total_tokens + doc_tokens <= max_tokens:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break


        response = client.chat.completions.create(
            model=CONFIG["model_id"],
            messages=[{
                "role": "system",
                "content": f"""(
            Generate {Country} structured Foreign/domestic Information Manipulation and Interference (FIMI) intelligence report related to the upcoming presidential elections:

            - Provide general context and identify key narratives all related to the upcoming elections with the reference documents and URLs as evidence.
            - Map these narratives lifecycle: First Detected {cluster_data['Timestamp'].min().strftime('%Y-%m-%d %H:%M')} → Last Updated {cluster_data['Timestamp'].max().strftime('%Y-%m-%d %H:%M')}.
            - Identify any narratives with mentions of anti-West, anti-France, pro/anti-ECOWAS, pro/anti-AES (Alliance of Sahel States), pro-Russia or Pro-China sentiment in respective to the upcoming elections.
            - Always in respective to the upcoming elections, clearly identify negative stereotyping, toxic incitement and mention some of them. Highlight and mention also the corresponding trigger lexicons used and provide URLs as evidence.
            - Identify coordinated network of accounts, highlight coordination signs like post timing, source distribution, inauthentic engagement spikes on posts. As metrics we have: Total Posts: {metrics.get('cumulative_activity', 'N/A')}, Peak Hourly Activity: {metrics.get('peak_activity', 'N/A')}, source_count: {cluster_data['Source'].nunique()}, Current Momentum Score: {metrics.get('momentum', 'N/A'):.2f}, Timestamp: {cluster_data['Timestamp']}.
            - Identify and analyse crossposting clusters.
            - Identify reused/manipulated media (e.g., repurposed protest footage from 2021–2024 framed as “current unrest,” AI-generated imagery of alleged government corruption).
            - Identify instances where the same content (text, images, videos) is being used directly repeatedly not as a retweet, if its a retweet specifically mention that, Provide specific examples and URLs.
            - Popular discussions, hashtags, or key phrases gaining traction, provide specific examples and URLs.
            - Main sources and influencers driving these conversations.
            - Analyze how the reused content is being used in different contexts.

            - Based on all above, suggest 2-3 Investigative leads using using clear, technical and advanced style sentences.
             Exclude speculation, history, or general claims. Reference only exact documented evidence with URLs. Focus strictly on election-related content. Skip reporting if no relevant narratives are found. Don't duplicate findings from the same documents you are analyzing.

            )
            """
            }, {
                "role": "user",
                "content": "\n".join([f"Document {i+1}: {doc[0]}\nURL: {doc[1]}\n[TIMESTAMP]: {doc[2]}" for i, doc in enumerate(selected_docs)])
            }],
            temperature=0.6,
            max_tokens=800
            )

        return {
            "report": response.choices[0].message.content,
            "metrics": metrics,
            "sample_texts": [doc[0] for doc in selected_docs],
            "sample_urls": [doc[1] for doc in selected_docs],
            "Time": [doc[2] for doc in selected_docs],
            "all_urls": cluster_data['URL'].head(20).tolist(),
            "source_count": cluster_data['Source'].nunique(),
            "momentum_score": cluster_data['momentum_score'].iloc[0]
        }

    except Exception as e:
        logger.error(f"Report generation failed: {str(e)}")
        return {"error": str(e)}

def categorize_momentum(score):
    score=float(score)
    if score <= 150:
        return 'Tier 1: Ambient Noise (Normal baseline activity)'
    elif score <= 500:
        return 'Tier 2: Emerging Narrative (Potential story development)'
    elif score <= 2000:
        return 'Tier 3: Coordinated Activity (Organized group behavior)'
    else:
        return 'Tier 4: Viral Emergency (Requires immediate response)'
