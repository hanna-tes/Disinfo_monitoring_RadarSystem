# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hkSMCjYIaZQcce1JDJS_IrJIqPLDnSw
"""

# pipeline.py
import torch
torch._dynamo.config.suppress_errors = True  # Disable Dynamo error propagation
#torch._dynamo.config.disable = True  # Completely disable TorchDynamo
#torch.set_float32_matmul_precision('high')  # Optimize for MPS/MacOS
import os
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, GPT2Tokenizer
from sklearn.decomposition import PCA
import annoy
from bertopic import BERTopic
from hdbscan import HDBSCAN
import logging
from groq import Groq
import re
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import random


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Configuration (now using Streamlit secrets)
CONFIG = {
    "model_id": "llama3-70b-8192",#qwen-qwq-32b",#"llama-3.3-70b-versatile",
    "gpu_params": {
        "batch_size": 256,  # Increased batch size
        "max_seq_length": 128,
        "num_workers": 4,
        "fp16": True  # Enable mixed precision
    },
    "bertrend": {
        "model_name": "bert-base-multilingual-cased",
        "temporal_weight": 0.4,
        "cluster_threshold": 0.35,
        "min_cluster_size": 4,
        "growth_threshold": 50,
        "pca_components": 32,  # Further reduced dimensions
        "chunk_size": 500,    # Larger processing chunks
        "ann_neighbors": 25,   # Reduced ANN neighbors
        "time_window_hours": 48  # Temporal constraint
    },
    "analysis": {
        "time_window": "24H",
        "min_sources": 5,
        "decay_factor": 0.015,
        "decay_power": 1.8,
        "visualization": {
        "plot_size": (12, 8),
        "palette": "viridis",
        "max_display_clusters": 10,
        "save_path": "./data/visualizations"
        }
    }
}

# Initialize Groq client with Streamlit secrets
def get_groq_client():
    return Groq(api_key=st.secrets.groq.api_key)

# Load BERT model to GPU
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)

# Initialize GPU with mixed precision
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = torch.cuda.amp.GradScaler(enabled=CONFIG["gpu_params"]["fp16"])
logger.info(f"Using device: {device}")

# Optimized BERT Model Loading
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)
bert_model = torch.compile(bert_model)  # Enable model compilation

# GPU-optimized Dataset with Pre-batching
class DRCDataset(Dataset):
    def __init__(self, texts):
        # Ensure all texts are strings and filter invalid entries
        self.texts = [str(t) for t in texts if isinstance(t, (str, bytes))]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Return empty string for invalid indices
        if idx >= len(self):
            return ""
        return self.texts[idx]

    def collate_fn(self, batch):
        # Filter empty strings and None values
        batch = [text for text in batch if isinstance(text, (str, bytes)) and len(text) > 0]
        
        # Handle empty batches
        if not batch:
            return {"input_ids": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
        
        # Tokenize with error handling
        try:
            return tokenizer(
                batch,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=CONFIG["gpu_params"]["max_seq_length"],
                return_attention_mask=True
            )
        except Exception as e:
            logger.error(f"Tokenizer error: {str(e)}")
            return {"input_ids": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
                    
# Turbo-charged BERT Embeddings Generator
def get_bert_embeddings(texts):
    dataset = DRCDataset(texts)
    dataloader = DataLoader(
        dataset,
        batch_size=CONFIG["gpu_params"]["batch_size"],
        num_workers=0,  # Disable multiprocessing for stability
        collate_fn=dataset.collate_fn,
        pin_memory=True,
        shuffle=False
    )

    embeddings = []
    with torch.no_grad():
        for batch in dataloader:
            # Skip empty batches
            if batch["input_ids"].shape[0] == 0:
                continue
                
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = bert_model(**inputs)
            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu())

    # Combine and reduce dimensionality on GPU
    full_embeddings = torch.cat(embeddings)
    pca = PCA(n_components=CONFIG["bertrend"]["pca_components"])
    return pca.fit_transform(full_embeddings.cpu().numpy())

# GPU-accelerated Temporal-Spatial Distance Calculator
def temporal_distance_matrix(embeddings, timestamps):
    """Hybrid distance calculation with temporal constraints"""
    # Convert to tensors on GPU
    emb_tensor = torch.tensor(embeddings, device=device)
    time_tensor = torch.tensor(timestamps, dtype=torch.float64, device=device)

    # Calculate time differences in hours
    time_diff = torch.abs(time_tensor[:, None] - time_tensor[None, :]) / 3.6e9

    # Apply temporal window mask
    time_mask = (time_diff < CONFIG["bertrend"]["time_window_hours"]).float()

    # Calculate semantic distances
    semantic_dists = torch.cdist(emb_tensor, emb_tensor, p=2)

    # Combine with temporal mask
    combined_dists = (
        CONFIG["bertrend"]["temporal_weight"] * time_diff +
        (1 - CONFIG["bertrend"]["temporal_weight"]) * semantic_dists
    ) * time_mask

    return combined_dists.cpu().numpy()

def preprocess_texts(texts):
    valid_texts = []
    for t in texts:
        # Handle NaN/None
        if pd.isna(t):
            continue
        # Convert bytes to string
        if isinstance(t, bytes):
            try:
                t = t.decode("utf-8")
            except UnicodeDecodeError:
                continue
        # Remove invalid characters
        t = re.sub(r"[\x00-\x1F\x7F-\x9F]", "", str(t))
        if len(t) > 0:
            valid_texts.append(t)
    return valid_texts
    
# Hyper-optimized BERTrend Analysis
def bertrend_analysis(df):
    """GPU-powered clustering pipeline with temporal constraints"""
    
    valid_df = df.dropna(subset=["text"])
    valid_texts = preprocess_texts(valid_df["text"].tolist())
    
    logger.info("Generating turbo-charged BERT embeddings...")
    embeddings = get_bert_embeddings(valid_texts)
    timestamps = df['Timestamp'].astype(np.int64).values

    # Create ANN index for fast neighbor search
    ann_index = annoy.AnnoyIndex(embeddings.shape[1], 'euclidean')
    for i, emb in enumerate(embeddings):
        ann_index.add_item(i, emb)
    ann_index.build(20)  # More trees for better accuracy

    # Initialize cluster array
    clusters = np.full(len(df), -1, dtype=int)
    current_cluster = 0

    # Process in large chunks with temporal constraints
    chunk_size = CONFIG["bertrend"]["chunk_size"]
    for i in range(0, len(embeddings), chunk_size):
        chunk_end = min(i + chunk_size, len(embeddings))
        logger.info(f"Processing chunk {i//chunk_size + 1}/{(len(embeddings)//chunk_size)+1}")

        # Get temporal window for current chunk
        chunk_times = timestamps[i:chunk_end]
        time_mask = (timestamps >= chunk_times[0]) & (timestamps <= chunk_times[-1])

        # Find ANN neighbors within temporal window
        neighbor_candidates = set()
        for idx in range(i, chunk_end):
            neighbors = ann_index.get_nns_by_item(
                idx,
                CONFIG["bertrend"]["ann_neighbors"],
                search_k=100000  # Higher search effort
            )
            neighbor_candidates.update(neighbors)

        # Filter candidates by temporal window
        candidate_indices = np.array(list(neighbor_candidates))
        candidate_indices = candidate_indices[time_mask[candidate_indices]]

        # Early exit for no candidates
        if len(candidate_indices) == 0:
            continue

        # Calculate distances only for candidates
        sub_emb = embeddings[candidate_indices]
        sub_ts = timestamps[candidate_indices]
        dist_matrix = temporal_distance_matrix(sub_emb, sub_ts)
        dist_matrix = dist_matrix.astype(np.double)

        # Fast GPU-accelerated clustering
        clusterer = HDBSCAN(
            min_cluster_size=CONFIG["bertrend"]["min_cluster_size"],
            metric="precomputed",
            cluster_selection_epsilon=CONFIG["bertrend"]["cluster_threshold"],
            core_dist_n_jobs=4
        )
        chunk_clusters = clusterer.fit_predict(dist_matrix)

        # Update cluster assignments
        valid_mask = chunk_clusters != -1
        chunk_clusters[valid_mask] += current_cluster
        clusters[candidate_indices] = chunk_clusters
        current_cluster = chunk_clusters[valid_mask].max() + 1 if valid_mask.any() else current_cluster

    df['Cluster'] = clusters
    return df[df['Cluster'] != -1]

# Optimized Momentum Calculator
def calculate_trend_momentum(clustered_df):
    """Optimized momentum calculation with source tracking"""
    df = clustered_df.copy()

    # Temporal binning
    df['time_window'] = df['Timestamp'].dt.floor(CONFIG["analysis"]["time_window"])

    # Aggregate with source tracking
    grouped = df.groupby(['Cluster', 'time_window']).agg(
        count=('text', 'size'),
        sources=('Source', 'unique'),
        last_time=('Timestamp', 'max')
    ).reset_index()

    # Calculate cumulative sources and momentum
    emerging = []
    momentum_states = {}

    for cluster, cluster_group in grouped.groupby('Cluster'):
        if cluster == -1:
            continue

        cluster_group = cluster_group.sort_values('time_window')
        cumulative_sources = set()
        momentum = 0
        last_update = None
        cumulative_activity = 0

        for idx, row in cluster_group.iterrows():

            cumulative_activity += row['count']
            # Time decay calculation
            if last_update is not None:
                delta_hours = (row['last_time'] - last_update).total_seconds() / 3600
                decay = np.exp(-CONFIG["analysis"]["decay_factor"] *
                             (delta_hours ** CONFIG["analysis"]["decay_power"]))
                momentum *= decay

            # Update momentum and sources
            momentum += row['count']
            cumulative_sources.update(row['sources'])

            # Calculate momentum score
            momentum_score = momentum * len(cumulative_sources) * np.log1p(row['count'])

            #if (momentum_score > CONFIG["bertrend"]["growth_threshold"] and
            #    len(cumulative_sources) >= CONFIG["analysis"]["min_sources"]):
            #    emerging.append((cluster, momentum_score))

            last_update = row['last_time']

        if (momentum_score > CONFIG["bertrend"]["growth_threshold"] and
                len(cumulative_sources) >= CONFIG["analysis"]["min_sources"]):
                emerging.append((cluster, momentum_score))

        momentum_states[cluster] = {
            'momentum': momentum_score,
            'cumulative_activity': cumulative_activity,  # Total posts
            'peak_activity': cluster_group['count'].max(),  # Highest single-period count
            'last_update': last_update,
            'sources': cumulative_sources
        }

    return sorted(emerging, key=lambda x: -x[1]), momentum_states

def visualize_trends(clustered_df, momentum_states, save_path=None):
    """Generate interactive trend visualizations with enhanced metrics"""
    plt.figure(figsize=CONFIG["analysis"]["visualization"]["plot_size"])

    # Timeline Plot with Dual Metrics
    plt.subplot(2, 1, 1)
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    top_clusters = sorted(
        momentum_states.items(),
        key=lambda x: -x[1]['cumulative_activity']
    )[:CONFIG["analysis"]["visualization"]["max_display_clusters"]]

    for cluster, metrics in top_clusters:
        cluster_data = clustered_df[clustered_df['Cluster'] == cluster]

        # Cumulative Activity (Bars)
        timeline = cluster_data.groupby(pd.Grouper(key='Timestamp', freq='24H'))['text'].count().cumsum()
        ax1.bar(timeline.index, timeline, alpha=0.3, label=f"Cluster {cluster} Total")

        # Momentum Score (Line)
        momentum_line = cluster_data.groupby(pd.Grouper(key='Timestamp', freq='24H'))['text'].count().rolling(4).mean()
        ax2.plot(momentum_line.index, momentum_line, lw=2, label=f"Cluster {cluster} Momentum")

    ax1.set_ylabel("Cumulative Posts")
    ax2.set_ylabel("4h Rolling Momentum")
    ax1.set_title("Narrative Growth vs Momentum Intensity")
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

    # Enhanced Heatmap with Activity Metrics
    plt.subplot(2, 1, 2)
    heatmap_data = clustered_df.pivot_table(
        index=pd.Grouper(key='Timestamp', freq='24H'),
        columns='Cluster',
        values='text',
        aggfunc='count',
        fill_value=0
    ).iloc[:, :CONFIG["analysis"]["visualization"]["max_display_clusters"]]

    sns.heatmap(
        heatmap_data.T,
        cmap=CONFIG["analysis"]["visualization"]["palette"],
        cbar_kws={'label': 'Activity Level'},
        annot=True,  # Add count annotations
        fmt="d"      # Integer formatting
    )
    plt.title("Cluster Activity Patterns with Post Counts")
    plt.xlabel("Time Windows")
    plt.ylabel("Cluster ID")

    plt.tight_layout()
    
    #if not save_path:
        #save_path = CONFIG["analysis"]["visualization"]["save_path"]
    save_path = os.path.abspath(
        save_path or CONFIG["analysis"]["visualization"]["save_path"]
    )
    os.makedirs(save_path, exist_ok=True)
    plot_path = os.path.join(save_path, "trend_visualization.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close()

    logger.info(f"Enhanced visualization saved to {plot_path}")
    return plot_path

def generate_investigative_report(cluster_data, momentum_states, cluster_id, keywords=None, max_tokens=1024):
    """Generate Gabon election monitoring report, respecting token limits."""
    client = Groq(api_key=CONFIG["api_key"])  # Make sure CONFIG is defined

    try:
        metrics = momentum_states.get(cluster_id, {})
        sample_docs = cluster_data[['text', 'URL', 'Timestamp']].values.tolist()

        random.shuffle(sample_docs)

        selected_docs = []
        total_tokens = 0

        # ***Modified Prompt Template for Gabon Election Monitoring (with borrowed elements)***
        summary_prompt_parts = [
            f"""
            Generate Gabon structured election IMI(Information manipulation and interference) intelligence report:

            - Provide general context and identify key narratives all related to the upcoming elections with the reference documents and URLs as evidence.
            - Map these narratives lifecycle: First Detected {cluster_data['Timestamp'].min().strftime('%Y-%m-%d %H:%M')} → Last Updated {cluster_data['Timestamp'].max().strftime('%Y-%m-%d %H:%M')}.
            - Identify and analyse ties and involvement of Russia or China always in respective to the upcoming elections.
            - Identify any narratives with mentions of anti-West, anti-France, pro/anti-ECOWAS, pro/anti-AES (Alliance of Sahel States), pro-Russia or Pro-China sentiment in respective to the upcoming elections.
            - Always in respective to the upcoming elections, clearly identify negative stereotyping, toxic incitement and mention some of them. Highlight and mention also the corresponding trigger lexicons used and provide URLs as evidence.
            - Identify coordinated network of accounts, highlight coordination signs like post timing, source distribution, inauthentic engagement spikes on posts. As metrics we have: Total Posts: {metrics.get('cumulative_activity', 'N/A')}, Peak Hourly Activity: {metrics.get('peak_activity', 'N/A')}, source_count: {cluster_data['Source'].nunique()}, Current Momentum Score: {metrics.get('momentum', 'N/A'):.2f}, Timestamp: {cluster_data['Timestamp']}.
            - Identify and analyse crossposting clusters.
            - Identify reused/manipulated media (e.g., repurposed protest footage from 2021–2024 framed as “current unrest,” AI-generated imagery of alleged government corruption).
            - Identify these narratives vehicles like memes, videos or text posts and provide the reference documents.
            - Identify AI-generated contents mimicking authentic Source.
            - Identify Linguistic fingerprints like translation artifacts, atypical local dialect usage.

            -Based on all above, suggest 2-3 Investigative leads using using clear, technical and advanced style sentences.

            - Analyze social media data and produce a structured investigative report covering the following areas:

                2. Content Patterns - Reused text/media/assets:
                - Identify instances where the same content (text, images, videos) is being used directly repeatedly not as a retweet, if its a retweet specifically mention that.
                - Provide specific examples and URLs.
                - Analyze how the reused content is being used in different contexts.

            Exclude: Speculation, unverified claims, historical background, general statements, findings or answers. Base findings only on provided evidence documents.
            Don't include other informations besides what's requested.
            Don't duplicate findings from the same documents you are analyzing. Only report NEW patterns not seen in previous analysis.
            Don't use bullet points in the report, only paragraphs: the focus points above are to orient the content of your report not to be used as bullet points.
            Document only what you have found and skip what you didn't find.
            Skip cases where you didn't find at least narratives aligned with the above request.
            Always reference your findings with documents URLs as evidence.
            Reference specific evidence from provided URLs.

            **Filter out posts that are clearly unrelated to election conversations, such as general advertisements. Focus on posts that discuss election impacts, policies, or related political issues in Gabon.**

            Documents:
            """
            }, {
                "role": "user",
                "content": "\n".join([f"Document {i+1}: {doc[0]}\nURL: {doc[1]}\n[TIMESTAMP]: {doc[2]}" for i, doc in enumerate(selected_docs)])
            }],
            temperature=0.6,
            max_tokens=800
            )

        return {
            "report": response.choices[0].message.content,
            "metrics": metrics,
            "sample_texts": [doc[0] for doc in selected_docs],
            "sample_urls": [doc[1] for doc in selected_docs],
            "Time": [doc[2] for doc in selected_docs],
            "all_urls": cluster_data['URL'].head(20).tolist(),
            "source_count": cluster_data['Source'].nunique(),
            "momentum_score": cluster_data['momentum_score'].iloc[0]
        }

    except Exception as e:
        logger.error(f"Report generation failed: {str(e)}")
        return {"error": str(e)}

def categorize_momentum(score):
    score=float(score)
    if score <= 150:
        return 'Tier 1: Ambient Noise (Normal baseline activity)'
    elif score <= 500:
        return 'Tier 2: Emerging Narrative (Potential story development)'
    elif score <= 2000:
        return 'Tier 3: Coordinated Activity (Organized group behavior)'
    else:
        return 'Tier 4: Viral Emergency (Requires immediate response)'
