# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-hkSMCjYIaZQcce1JDJS_IrJIqPLDnSw
"""

# pipeline.py
import torch
torch._dynamo.config.suppress_errors = True  # Disable Dynamo error propagation
#torch._dynamo.config.disable = True  # Completely disable TorchDynamo
#torch.set_float32_matmul_precision('high')  # Optimize for MPS/MacOS
import os
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, GPT2Tokenizer
from sklearn.decomposition import PCA
import annoy
from bertopic import BERTopic
from hdbscan import HDBSCAN
import logging
from groq import Groq
import re
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import random


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Configuration (now using Streamlit secrets)
CONFIG = {
    "model_id": "llama3-70b-8192",#qwen-qwq-32b",#"llama-3.3-70b-versatile",
    "gpu_params": {
        "batch_size": 256,  # Increased batch size
        "max_seq_length": 128,
        "num_workers": 4,
        "fp16": True  # Enable mixed precision
    },
    "bertrend": {
        "model_name": "bert-base-multilingual-cased",
        "temporal_weight": 0.4,
        "cluster_threshold": 0.35,
        "min_cluster_size": 4,
        "growth_threshold": 50,
        "pca_components": 32,  # Further reduced dimensions
        "chunk_size": 500,    # Larger processing chunks
        "ann_neighbors": 25,   # Reduced ANN neighbors
        "time_window_hours": 48  # Temporal constraint
    },
    "analysis": {
        "time_window": "24H",
        "min_sources": 5,
        "decay_factor": 0.015,
        "decay_power": 1.8,
        "visualization": {
        "plot_size": (12, 8),
        "palette": "viridis",
        "max_display_clusters": 10,
        "save_path": "./data/visualizations"
        }
    }
}

# Initialize Groq client with Streamlit secrets
def get_groq_client():
    return Groq(api_key=st.secrets.groq.api_key)

# Load BERT model to GPU
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)

# Initialize GPU with mixed precision
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
scaler = torch.cuda.amp.GradScaler(enabled=CONFIG["gpu_params"]["fp16"])
logger.info(f"Using device: {device}")

# Optimized BERT Model Loading
tokenizer = BertTokenizer.from_pretrained(CONFIG["bertrend"]["model_name"])
bert_model = BertModel.from_pretrained(CONFIG["bertrend"]["model_name"]).to(device)
bert_model = torch.compile(bert_model)  # Enable model compilation

# GPU-optimized Dataset with Pre-batching
class DRCDataset(Dataset):
    def __init__(self, texts):
        # Ensure all texts are strings and filter invalid entries
        self.texts = [str(t) for t in texts if isinstance(t, (str, bytes))]

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        # Return empty string for invalid indices
        if idx >= len(self):
            return ""
        return self.texts[idx]

    def collate_fn(self, batch):
        # Filter empty strings and None values
        batch = [text for text in batch if isinstance(text, (str, bytes)) and len(text) > 0]
        
        # Handle empty batches
        if not batch:
            return {"input_ids": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((0, CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
        
        # Tokenize with error handling
        try:
            return tokenizer(
                batch,
                return_tensors="pt",
                padding="max_length",
                truncation=True,
                max_length=CONFIG["gpu_params"]["max_seq_length"],
                return_attention_mask=True
            )
        except Exception as e:
            logger.error(f"Tokenizer error: {str(e)}")
            return {"input_ids": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long),
                    "attention_mask": torch.zeros((len(batch), CONFIG["gpu_params"]["max_seq_length"]), dtype=torch.long)}
                    
# Turbo-charged BERT Embeddings Generator
def get_bert_embeddings(texts):
    dataset = DRCDataset(texts)
    dataloader = DataLoader(
        dataset,
        batch_size=CONFIG["gpu_params"]["batch_size"],
        num_workers=0,  # Disable multiprocessing for stability
        collate_fn=dataset.collate_fn,
        pin_memory=True,
        shuffle=False
    )

    embeddings = []
    with torch.no_grad():
        for batch in dataloader:
            # Skip empty batches
            if batch["input_ids"].shape[0] == 0:
                continue
                
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = bert_model(**inputs)
            embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu())

    # Combine and reduce dimensionality on GPU
    full_embeddings = torch.cat(embeddings)
    pca = PCA(n_components=CONFIG["bertrend"]["pca_components"])
    return pca.fit_transform(full_embeddings.cpu().numpy())

# GPU-accelerated Temporal-Spatial Distance Calculator
def temporal_distance_matrix(embeddings, timestamps):
    """Hybrid distance calculation with temporal constraints"""
    # Convert to tensors on GPU
    emb_tensor = torch.tensor(embeddings, device=device)
    time_tensor = torch.tensor(timestamps, dtype=torch.float64, device=device)

    # Calculate time differences in hours
    time_diff = torch.abs(time_tensor[:, None] - time_tensor[None, :]) / 3.6e9

    # Apply temporal window mask
    time_mask = (time_diff < CONFIG["bertrend"]["time_window_hours"]).float()

    # Calculate semantic distances
    semantic_dists = torch.cdist(emb_tensor, emb_tensor, p=2)

    # Combine with temporal mask
    combined_dists = (
        CONFIG["bertrend"]["temporal_weight"] * time_diff +
        (1 - CONFIG["bertrend"]["temporal_weight"]) * semantic_dists
    ) * time_mask

    return combined_dists.cpu().numpy()

def preprocess_texts(texts):
    valid_texts = []
    for t in texts:
        # Handle NaN/None
        if pd.isna(t):
            continue
        # Convert bytes to string
        if isinstance(t, bytes):
            try:
                t = t.decode("utf-8")
            except UnicodeDecodeError:
                continue
        # Remove invalid characters
        t = re.sub(r"[\x00-\x1F\x7F-\x9F]", "", str(t))
        if len(t) > 0:
            valid_texts.append(t)
    return valid_texts
    
# Hyper-optimized BERTrend Analysis
def bertrend_analysis(df):
    """GPU-powered clustering pipeline with temporal constraints"""
    
    valid_df = df.dropna(subset=["text"])
    valid_texts = preprocess_texts(valid_df["text"].tolist())
    
    logger.info("Generating turbo-charged BERT embeddings...")
    embeddings = get_bert_embeddings(valid_texts)
    timestamps = df['Timestamp'].astype(np.int64).values

    # Create ANN index for fast neighbor search
    ann_index = annoy.AnnoyIndex(embeddings.shape[1], 'euclidean')
    for i, emb in enumerate(embeddings):
        ann_index.add_item(i, emb)
    ann_index.build(20)  # More trees for better accuracy

    # Initialize cluster array
    clusters = np.full(len(df), -1, dtype=int)
    current_cluster = 0

    # Process in large chunks with temporal constraints
    chunk_size = CONFIG["bertrend"]["chunk_size"]
    for i in range(0, len(embeddings), chunk_size):
        chunk_end = min(i + chunk_size, len(embeddings))
        logger.info(f"Processing chunk {i//chunk_size + 1}/{(len(embeddings)//chunk_size)+1}")

        # Get temporal window for current chunk
        chunk_times = timestamps[i:chunk_end]
        time_mask = (timestamps >= chunk_times[0]) & (timestamps <= chunk_times[-1])

        # Find ANN neighbors within temporal window
        neighbor_candidates = set()
        for idx in range(i, chunk_end):
            neighbors = ann_index.get_nns_by_item(
                idx,
                CONFIG["bertrend"]["ann_neighbors"],
                search_k=100000  # Higher search effort
            )
            neighbor_candidates.update(neighbors)

        # Filter candidates by temporal window
        candidate_indices = np.array(list(neighbor_candidates))
        candidate_indices = candidate_indices[time_mask[candidate_indices]]

        # Early exit for no candidates
        if len(candidate_indices) == 0:
            continue

        # Calculate distances only for candidates
        sub_emb = embeddings[candidate_indices]
        sub_ts = timestamps[candidate_indices]
        dist_matrix = temporal_distance_matrix(sub_emb, sub_ts)
        dist_matrix = dist_matrix.astype(np.double)

        # Fast GPU-accelerated clustering
        clusterer = HDBSCAN(
            min_cluster_size=CONFIG["bertrend"]["min_cluster_size"],
            metric="precomputed",
            cluster_selection_epsilon=CONFIG["bertrend"]["cluster_threshold"],
            core_dist_n_jobs=4
        )
        chunk_clusters = clusterer.fit_predict(dist_matrix)

        # Update cluster assignments
        valid_mask = chunk_clusters != -1
        chunk_clusters[valid_mask] += current_cluster
        clusters[candidate_indices] = chunk_clusters
        current_cluster = chunk_clusters[valid_mask].max() + 1 if valid_mask.any() else current_cluster

    df['Cluster'] = clusters
    return df[df['Cluster'] != -1]

# Optimized Momentum Calculator
def calculate_trend_momentum(clustered_df):
    """Optimized momentum calculation with source tracking"""
    df = clustered_df.copy()

    # Temporal binning
    df['time_window'] = df['Timestamp'].dt.floor(CONFIG["analysis"]["time_window"])

    # Aggregate with source tracking
    grouped = df.groupby(['Cluster', 'time_window']).agg(
        count=('text', 'size'),
        sources=('Source', 'unique'),
        last_time=('Timestamp', 'max')
    ).reset_index()

    # Calculate cumulative sources and momentum
    emerging = []
    momentum_states = {}

    for cluster, cluster_group in grouped.groupby('Cluster'):
        if cluster == -1:
            continue

        cluster_group = cluster_group.sort_values('time_window')
        cumulative_sources = set()
        momentum = 0
        last_update = None
        cumulative_activity = 0

        for idx, row in cluster_group.iterrows():

            cumulative_activity += row['count']
            # Time decay calculation
            if last_update is not None:
                delta_hours = (row['last_time'] - last_update).total_seconds() / 3600
                decay = np.exp(-CONFIG["analysis"]["decay_factor"] *
                             (delta_hours ** CONFIG["analysis"]["decay_power"]))
                momentum *= decay

            # Update momentum and sources
            momentum += row['count']
            cumulative_sources.update(row['sources'])

            # Calculate momentum score
            momentum_score = momentum * len(cumulative_sources) * np.log1p(row['count'])

            #if (momentum_score > CONFIG["bertrend"]["growth_threshold"] and
            #    len(cumulative_sources) >= CONFIG["analysis"]["min_sources"]):
            #    emerging.append((cluster, momentum_score))

            last_update = row['last_time']

        if (momentum_score > CONFIG["bertrend"]["growth_threshold"] and
                len(cumulative_sources) >= CONFIG["analysis"]["min_sources"]):
                emerging.append((cluster, momentum_score))

        momentum_states[cluster] = {
            'momentum': momentum_score,
            'cumulative_activity': cumulative_activity,  # Total posts
            'peak_activity': cluster_group['count'].max(),  # Highest single-period count
            'last_update': last_update,
            'sources': cumulative_sources
        }

    return sorted(emerging, key=lambda x: -x[1]), momentum_states

def visualize_trends(clustered_df, momentum_states, save_path=None):
    """Generate interactive trend visualizations with enhanced metrics"""
    plt.figure(figsize=CONFIG["analysis"]["visualization"]["plot_size"])

    # Timeline Plot with Dual Metrics
    plt.subplot(2, 1, 1)
    ax1 = plt.gca()
    ax2 = ax1.twinx()

    top_clusters = sorted(
        momentum_states.items(),
        key=lambda x: -x[1]['cumulative_activity']
    )[:CONFIG["analysis"]["visualization"]["max_display_clusters"]]

    for cluster, metrics in top_clusters:
        cluster_data = clustered_df[clustered_df['Cluster'] == cluster]

        # Cumulative Activity (Bars)
        timeline = cluster_data.groupby(pd.Grouper(key='Timestamp', freq='24H'))['text'].count().cumsum()
        ax1.bar(timeline.index, timeline, alpha=0.3, label=f"Cluster {cluster} Total")

        # Momentum Score (Line)
        momentum_line = cluster_data.groupby(pd.Grouper(key='Timestamp', freq='24H'))['text'].count().rolling(4).mean()
        ax2.plot(momentum_line.index, momentum_line, lw=2, label=f"Cluster {cluster} Momentum")

    ax1.set_ylabel("Cumulative Posts")
    ax2.set_ylabel("4h Rolling Momentum")
    ax1.set_title("Narrative Growth vs Momentum Intensity")
    ax1.legend(loc='upper left')
    ax2.legend(loc='upper right')
    plt.grid(True, alpha=0.3)

    # Enhanced Heatmap with Activity Metrics
    plt.subplot(2, 1, 2)
    heatmap_data = clustered_df.pivot_table(
        index=pd.Grouper(key='Timestamp', freq='24H'),
        columns='Cluster',
        values='text',
        aggfunc='count',
        fill_value=0
    ).iloc[:, :CONFIG["analysis"]["visualization"]["max_display_clusters"]]

    sns.heatmap(
        heatmap_data.T,
        cmap=CONFIG["analysis"]["visualization"]["palette"],
        cbar_kws={'label': 'Activity Level'},
        annot=True,  # Add count annotations
        fmt="d"      # Integer formatting
    )
    plt.title("Cluster Activity Patterns with Post Counts")
    plt.xlabel("Time Windows")
    plt.ylabel("Cluster ID")

    plt.tight_layout()
    
    #if not save_path:
        #save_path = CONFIG["analysis"]["visualization"]["save_path"]
    save_path = os.path.abspath(
        save_path or CONFIG["analysis"]["visualization"]["save_path"]
    )
    os.makedirs(save_path, exist_ok=True)
    plot_path = os.path.join(save_path, "trend_visualization.png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close()

    logger.info(f"Enhanced visualization saved to {plot_path}")
    return plot_path

def generate_investigative_report(cluster_data, momentum_states, cluster_id, max_tokens=1024):
    """Generate report with top 3 documents and their URLs"""
    client = get_groq_client()
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2-xl")

    try:
        # Get top 10 documents with URLs
        metrics = momentum_states.get(cluster_id, {})
        sample_docs = cluster_data[['text', 'URL', 'Timestamp']].values.tolist()

        random.shuffle(sample_docs)
        Country="Gabon"


        #report_context = f"""
        #Quantitative Context:
        #- Total Posts: {metrics.get('cumulative_activity', 'N/A')}
        #- Peak Hourly Activity: {metrics.get('peak_activity', 'N/A')}
        #- Unique Sources: {metrics.get('sources', 'N/A')}
        #- Current Momentum Score: {metrics.get('momentum', 'N/A'):.2f}
        #"""
        # Initialize the list of documents to include
        selected_docs = []
        total_tokens = 0

        # Select documents until we hit the token limit
        for doc in sample_docs:
            # Calculate the token count for the document
            doc_tokens = len(tokenizer.encode(doc[0]))  # Encoding only the text

            if total_tokens + doc_tokens <= max_tokens:
                selected_docs.append(doc)
                total_tokens += doc_tokens
            else:
                break


        response = client.chat.completions.create(
            model=CONFIG["model_id"],
            messages=[{
                "role": "system",
                "content": f"""(
                    Generate {Country} structured Foreign/domestic Information Manipulation and Interference (FIMI) intelligence report related to the upcoming presidential elections:

                    -Provide general context and identify key narratives with the reference documents as evidence.\n
                    -Map these narratives lifecycle: First Detected {cluster_data['Timestamp'].min().strftime('%Y-%m-%d %H:%M')} → Last Updated {cluster_data['Timestamp'].max().strftime('%Y-%m-%d %H:%M')}\n
                    -Identify these narratives vehicles like memes, videos or text posts and provide the reference documents\n
                    -Identify primary sources platforms used to spread these narratives\n


                    -Identify and analyse ties and involvement of Russia or China or Turkey or Saudi Arabia.\n
                    -Identify extremism/jihadist cases\n
                    -Identify any case of anti-West, anti-France, pro/anti-ECOWAS, pro/anti-AES (Alliance of Sahel States), pro-Russia or Pro-China sentiment\n
                    -Clearly identify hate speech, negative stereotyping, toxic incitement and mention some of them. Highlight and mention also the corresponding trigger lexicons used\n\n


                    - Identify coordinated network of accounts, analyse network topology and highlight coordination signs like post timing, source distribution, inauthentic engagement spikes on posts. As metrics we have: Total Posts: {metrics.get('cumulative_activity', 'N/A')}, Peak Hourly Activity: {metrics.get('peak_activity', 'N/A')}, source_count: {cluster_data['Source'].nunique()}, Current Momentum Score: {metrics.get('momentum', 'N/A'):.2f}, Timestamp: {cluster_data['Timestamp']}\n
                    - Identify and analyse crossposting clusters\n
                    - Identify narrative engineering like story arc development, meme warfare tactics, sentiment manipulation techniques\n
                    - Identify AI-generated contents mimicking authentic Source\n
                    - Identify reused/manipulated media (e.g., repurposed protest footage from 2021–2024 framed as “current unrest,” AI-generated imagery of alleged government corruption); Identify Viral templates linking policy decisions (e.g., austerity, resource deals) to foreign actors (France/UAE/China/Turkey/Saudi Arabia)\n
                    - Identify Linguistic fingerprints like translation artifacts, atypical local dialect usage\n


                    -Based on all above, suggest 2-3 strong online Investigative leads using using clear, technical and advanced style sentences\n\n
                    Exclude: Speculation, unverified claims, historical background, general statements, findings or answers. Base findings only on provided evidence documents\n
                    Don't include other informations besides what's requested.\n
                    All above insights should be provided relatively to the upcoming presidential elections. Therefore, skip and remove or just add 'non related' on cases or insights or narratives or any patterns that are not related to the upcoming elections.\n
                    Don't duplicate findings from the same documents you are analyzing. Only report NEW patterns not seen in previous analysis.\n
                    Don't use bullet points in the report, only paragraphs: the focus points above are to orient the content of your report not to be used as bullet points.\n
                    Document only what you have found and skip what you didn't find.\n
                    Skip and remove cases or insights or narratives or any patterns that are not related to the upcoming elections.\n
                    Always reference your findings with documents URLs as evidence.\n
                    Reference specific evidence from provided URLs
                )
                """
            }, {
                "role": "user",
                "content": "\n".join([f"Document {i+1}: {doc[0]}\nURL: {doc[1]}\n[TIMESTAMP]: {doc[2]}" for i, doc in enumerate(selected_docs)])
            }],
            temperature=0.6,
            max_tokens=800
            )

        return {
            "report": response.choices[0].message.content,
            "metrics": metrics,
            "sample_texts": [doc[0] for doc in selected_docs],
            "sample_urls": [doc[1] for doc in selected_docs],
            "Time": [doc[2] for doc in selected_docs],
            "all_urls": cluster_data['URL'].head(20).tolist(),
            "source_count": cluster_data['Source'].nunique(),
            "momentum_score": cluster_data['momentum_score'].iloc[0]
        }

    except Exception as e:
        logger.error(f"Report generation failed: {str(e)}")
        return {"error": str(e)}

def categorize_momentum(score):
    score=float(score)
    if score <= 150:
        return 'Tier 1: Ambient Noise (Normal baseline activity)'
    elif score <= 500:
        return 'Tier 2: Emerging Narrative (Potential story development)'
    elif score <= 2000:
        return 'Tier 3: Coordinated Activity (Organized group behavior)'
    else:
        return 'Tier 4: Viral Emergency (Requires immediate response)'